{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjOcagw2wWHSCaTv8yQ9Lu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iHakawaTi/web-scraping-project/blob/main/web_scraping_bs4_praw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Part 1: Web Scraping with BeautifulSoup ---"
      ],
      "metadata": {
        "id": "Rt8sCHjefITk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWepsBMbG3jP"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_url = 'https://www.careopinion.org.uk/opinions/?page={}'\n",
        "base_domain = 'https://www.careopinion.org.uk'\n",
        "\n",
        "def scrape_page(url):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    soup = BeautifulSoup(response.content, 'lxml')\n",
        "    opinions_link = soup.find_all(\"a\", class_='font-c-1 tooltip')\n",
        "    story_titles = []\n",
        "    story_dates = []\n",
        "    story_opinions = []\n",
        "    story_services = []\n",
        "    story_summaries = []\n",
        "\n",
        "    for link in opinions_link:\n",
        "        opinion_title = link.get_text(strip=True).replace('\"', '')\n",
        "        story_titles.append(opinion_title)\n",
        "        href = link.get('href')\n",
        "        if href:\n",
        "            full_url = base_domain + href\n",
        "\n",
        "            opinion_response = requests.get(full_url)\n",
        "            opinion_response.raise_for_status()\n",
        "            opinion_soup = BeautifulSoup(opinion_response.content, 'lxml')\n",
        "\n",
        "            opinion_tag = opinion_soup.find('blockquote', id='opinion_body')\n",
        "            opinion_text = opinion_tag.get_text(strip=True) if opinion_tag else 'Story text does not exist.'\n",
        "            story_opinions.append(opinion_text)\n",
        "\n",
        "            date_tag = opinion_soup.find('time')\n",
        "            opinion_date = date_tag.get_text(strip=True) if date_tag else 'Date does not exist.'\n",
        "            story_dates.append(opinion_date)\n",
        "\n",
        "            service_tag = opinion_soup.find('p', class_='service_location m-margin-w-1')\n",
        "            opinion_related_service = service_tag.get_text(strip=True) if service_tag else 'Related Service does not exist.'\n",
        "            story_services.append(opinion_related_service)\n",
        "\n",
        "            summary_tag = opinion_soup.find('div',class_='inner')\n",
        "            opinion_summary = summary_tag.get_text(separator=' ',strip=True) if summary_tag else 'No summary'\n",
        "            story_summaries.append(opinion_summary)\n",
        "\n",
        "            time.sleep(1)\n",
        "\n",
        "    return story_titles, story_dates, story_opinions, story_services, story_summaries\n",
        "\n",
        "\n",
        "def scraping_all_pages(base_url, max_pages):\n",
        "    all_titles = []\n",
        "    all_dates = []\n",
        "    all_opinions = []\n",
        "    all_services = []\n",
        "    all_summaries = []\n",
        "\n",
        "    total_stories = 0\n",
        "\n",
        "    for page in range(1, max_pages + 1):\n",
        "        print(f\"Collecting data from page: {page}\")\n",
        "        page_url = base_url.format(page)\n",
        "        titles, dates, opinions, services, summaries = scrape_page(page_url)\n",
        "\n",
        "        all_titles.extend(titles)\n",
        "        all_dates.extend(dates)\n",
        "        all_opinions.extend(opinions)\n",
        "        all_services.extend(services)\n",
        "        all_summaries.extend(summaries)\n",
        "\n",
        "        total_stories += len(titles)\n",
        "        print(f\"Total stories: {total_stories}\")\n",
        "        if total_stories >= 100:\n",
        "            break\n",
        "\n",
        "        time.sleep(1)\n",
        "\n",
        "    return all_titles, all_dates, all_opinions, all_services, all_summaries\n",
        "\n",
        "\n",
        "titles, dates, opinions, services, summaries = scraping_all_pages(base_url, max_pages=25)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'Story Title': titles,\n",
        "    'Date Published': dates,\n",
        "    'Story Text': opinions,\n",
        "    'Related Service': services,\n",
        "    'Summary':summaries\n",
        "})\n",
        "\n",
        "df.to_csv('care_opinion.csv', index=False)\n",
        "print(\"Saved results to csv file.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gY3DjkGkvrmW",
        "outputId": "ffcdb36a-99ae-4dbf-8e75-86b463892a94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting data from page: 1\n",
            "Total stories: 6\n",
            "Collecting data from page: 2\n",
            "Total stories: 12\n",
            "Collecting data from page: 3\n",
            "Total stories: 18\n",
            "Collecting data from page: 4\n",
            "Total stories: 24\n",
            "Collecting data from page: 5\n",
            "Total stories: 30\n",
            "Collecting data from page: 6\n",
            "Total stories: 36\n",
            "Collecting data from page: 7\n",
            "Total stories: 42\n",
            "Collecting data from page: 8\n",
            "Total stories: 48\n",
            "Collecting data from page: 9\n",
            "Total stories: 54\n",
            "Collecting data from page: 10\n",
            "Total stories: 60\n",
            "Collecting data from page: 11\n",
            "Total stories: 66\n",
            "Collecting data from page: 12\n",
            "Total stories: 72\n",
            "Collecting data from page: 13\n",
            "Total stories: 78\n",
            "Collecting data from page: 14\n",
            "Total stories: 84\n",
            "Collecting data from page: 15\n",
            "Total stories: 90\n",
            "Collecting data from page: 16\n",
            "Total stories: 96\n",
            "Collecting data from page: 17\n",
            "Total stories: 102\n",
            "Saved results to csv file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install praw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJPSz49mKsjx",
        "outputId": "c48d3cab-7e34-4b19-8224-104847255d95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting prawcore<3,>=2.4 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update_checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.11/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.8.3)\n",
            "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Installing collected packages: update_checker, prawcore, praw\n",
            "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Part 2: Web Scraping with PRAW (Reddit API) ---"
      ],
      "metadata": {
        "id": "cnH5v-soe8uU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import csv\n",
        "import logging\n",
        "\n",
        "# Suppress PRAW warnings about asynchronous environment\n",
        "logging.getLogger('praw').setLevel(logging.ERROR)"
      ],
      "metadata": {
        "id": "uCOfSDbpHw55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reddit = praw.Reddit(\n",
        "    client_id='eQapkTFF6nCSbK3-_CFDgg',\n",
        "    client_secret='oamVjLeRyBzUAshqEWVGgloerFR0Ww',\n",
        "    user_agent='test'\n",
        ")\n",
        "\n",
        "keyword = 'hospital'\n",
        "subreddits = ['hospitals', 'nursing', 'medicine']\n",
        "limit_per_sub = 30\n",
        "posts_data = []\n",
        "for sub in subreddits:\n",
        "    subreddit = reddit.subreddit(sub)\n",
        "    print(f'Searching subreddit: {sub}')\n",
        "    for post in subreddit.search(keyword, sort='new', limit=limit_per_sub):\n",
        "        print(f'Processing post: {post.id}')\n",
        "\n",
        "        user_id = f'user_{post.author}' if post.author else 'Deleted'\n",
        "        post_content = f\"{post.title} {post.selftext}\"\n",
        "        keyword_found = keyword.lower() in post_content.lower()\n",
        "        number_replies=len(post.comments.list())\n",
        "\n",
        "        posts_data.append({\n",
        "            'Post_ID': post.id,\n",
        "            'Title': post.title,\n",
        "            'Content': post.selftext,\n",
        "            'Author': user_id,\n",
        "            'Subreddit': post.subreddit.display_name,\n",
        "            'Date': datetime.fromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'Num_Comments': post.num_comments,\n",
        "            'URL': post.url,\n",
        "            'Keyword_Found': keyword_found,\n",
        "            'subreddit':sub\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(posts_data)\n",
        "df.to_csv('reddit_topic_posts.csv', index=False, quoting=csv.QUOTE_ALL)\n",
        "\n",
        "print('Finished scraping Reddit posts.')\n",
        "#print(df.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BI-S_uzHwD5",
        "outputId": "abe5595f-c89a-4a98-ec7b-f6c410f5479e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching subreddit: hospitals\n",
            "Processing post: nk4vu3\n",
            "Processing post: mw6618\n",
            "Processing post: kk207b\n",
            "Processing post: kaukkg\n",
            "Processing post: jqxsrp\n",
            "Processing post: izdup1\n",
            "Processing post: izdiyt\n",
            "Processing post: hq8ttc\n",
            "Processing post: h8ntet\n",
            "Processing post: g7a28i\n",
            "Processing post: fxjgun\n",
            "Processing post: ft3yl6\n",
            "Processing post: fmumr1\n",
            "Processing post: fgzm16\n",
            "Processing post: fdrakw\n",
            "Processing post: f3n1ip\n",
            "Processing post: ednkid\n",
            "Processing post: edmcpy\n",
            "Processing post: edm5n8\n",
            "Processing post: edm1p8\n",
            "Processing post: edlvx6\n",
            "Processing post: edlra3\n",
            "Processing post: edlnqb\n",
            "Processing post: edlj4u\n",
            "Processing post: edleyz\n",
            "Processing post: edlae9\n",
            "Processing post: edl0zd\n",
            "Processing post: ed6ntd\n",
            "Processing post: ed6crm\n",
            "Processing post: ed5wbm\n",
            "Searching subreddit: nursing\n",
            "Processing post: 1mlpcd1\n",
            "Processing post: 1mlmtee\n",
            "Processing post: 1mlmm97\n",
            "Processing post: 1mlgyrs\n",
            "Processing post: 1mlfvyn\n",
            "Processing post: 1mlavgk\n",
            "Processing post: 1mlau8z\n",
            "Processing post: 1ml803u\n",
            "Processing post: 1ml6hu8\n",
            "Processing post: 1ml5pg3\n",
            "Processing post: 1ml3c6v\n",
            "Processing post: 1ml1rde\n",
            "Processing post: 1ml0e2d\n",
            "Processing post: 1mkzt5f\n",
            "Processing post: 1mkzh6m\n",
            "Processing post: 1mkxr0p\n",
            "Processing post: 1mkwxpu\n",
            "Processing post: 1mkwp3g\n",
            "Processing post: 1mkwjed\n",
            "Processing post: 1mktptz\n",
            "Processing post: 1mkr1kz\n",
            "Processing post: 1mkp7fv\n",
            "Processing post: 1mkonb6\n",
            "Processing post: 1mklgz9\n",
            "Processing post: 1mkkvlo\n",
            "Processing post: 1mkjvi8\n",
            "Processing post: 1mkis7j\n",
            "Processing post: 1mkin47\n",
            "Processing post: 1mkiacz\n",
            "Processing post: 1mkgesx\n",
            "Searching subreddit: medicine\n",
            "Processing post: 1mkxjko\n",
            "Processing post: 1mkdwjn\n",
            "Processing post: 1mi8w34\n",
            "Processing post: 1mh1v2s\n",
            "Processing post: 1mgs990\n",
            "Processing post: 1me4vo5\n",
            "Processing post: 1mdrtbb\n",
            "Processing post: 1m8k8m3\n",
            "Processing post: 1m8awjl\n",
            "Processing post: 1m84tab\n",
            "Processing post: 1m7bwby\n",
            "Processing post: 1m6qi9g\n",
            "Processing post: 1m5lpol\n",
            "Processing post: 1m59imk\n",
            "Processing post: 1m4524r\n",
            "Processing post: 1m3jg6u\n",
            "Processing post: 1m2bp2e\n",
            "Processing post: 1m0rtux\n",
            "Processing post: 1m00tyo\n",
            "Processing post: 1lxbxw2\n",
            "Processing post: 1lx5fd4\n",
            "Processing post: 1luwbtd\n",
            "Processing post: 1luvcgf\n",
            "Processing post: 1ltgy4b\n",
            "Processing post: 1lqsn0m\n",
            "Processing post: 1lqgi7o\n",
            "Processing post: 1lqg86g\n",
            "Processing post: 1lp3zrg\n",
            "Processing post: 1llb9ih\n",
            "Processing post: 1ll2usy\n",
            "Finished scraping Reddit posts.\n"
          ]
        }
      ]
    }
  ]
}